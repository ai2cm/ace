import argparse
import dataclasses
import os
import subprocess
from typing import List, Optional, Tuple

import dacite
import fsspec
import matplotlib.pyplot as plt
import numpy as np
import torch
import xarray as xr
import yaml
from matplotlib.axes import Axes

from fme.core import metrics

DATASETS_DIR = os.path.join(os.path.dirname(__file__), "interp-datasets")
OUT_DIR = os.path.join(os.path.dirname(__file__), "output")


@dataclasses.dataclass
class PlotVariable:
    name: str
    scale: float = 1.0
    bias_vmax: Optional[float] = None

    @property
    def bias_vmin(self) -> Optional[float]:
        if self.bias_vmax is None:
            return None
        return -self.bias_vmax


@dataclasses.dataclass
class LinearFitConfig:
    """
    Configuration for the linear fit.

    Parameters:
        i_start: The start index of the linear fit. By default
            this is the first index.
        i_end: The end index of the linear fit (non-inclusive). By default
            all values to the end are included.
    """

    i_start: Optional[int] = None
    i_end: Optional[int] = None


@dataclasses.dataclass
class Config:
    """
    Configuration for the analysis.

    Parameters:
        name: The name of the configuration.
        weights: The interpolant weights for each dataset.
        beaker_datasets: The names of the Beaker datasets to download.
        noise_floor_mean_dataset: The name of the noise floor mean dataset,
            as generated by the `noise_floor` scripts.
        noise_floor_std_dataset: The name of the noise floor std dataset,
            as generated by the `noise_floor` scripts.
        target_index: The index of the target interpolant used when computing
            bias maps.
        plot_variables: The variables to plot.
        inference_years: The number of years in the inference.
        linear_fit: The linear fit configuration.
    """

    name: str
    weights: List[float]
    beaker_datasets: List[str]
    noise_floor_mean_dataset: str
    noise_floor_std_dataset: str
    target_index: int
    plot_variables: List[PlotVariable]
    inference_years: int
    linear_fit: LinearFitConfig = dataclasses.field(default_factory=LinearFitConfig)

    def __post_init__(self):
        self._rmse_mean_ds: Optional[xr.Dataset] = None
        self._rmse_std_ds: Optional[xr.Dataset] = None

    @property
    def rmse_mean_ds(self) -> xr.Dataset:
        if self._rmse_mean_ds is None:
            self._rmse_mean_ds = xr.open_dataset(
                fsspec.open(self.noise_floor_mean_dataset).open()
            )
        return self._rmse_mean_ds

    @property
    def rmse_std_ds(self) -> xr.Dataset:
        if self._rmse_std_ds is None:
            self._rmse_std_ds = xr.open_dataset(
                fsspec.open(self.noise_floor_std_dataset).open()
            )
        return self._rmse_std_ds

    @property
    def output_dir(self) -> str:
        return os.path.join(OUT_DIR, self.name)

    @property
    def fit_slice(self) -> slice:
        return slice(self.linear_fit.i_start, self.linear_fit.i_end)


def download_beaker_time_mean_diagnostics(name: str):
    subprocess.check_call(
        [
            "beaker",
            "dataset",
            "fetch",
            name,
            "-o",
            os.path.join(DATASETS_DIR, name),
            "--prefix",
            "time_mean_diagnostics.nc",
        ]
    )


def compute_pattern_rmse(data: np.ndarray, area: xr.DataArray) -> float:
    return (
        metrics.weighted_mean(
            torch.square(torch.as_tensor(data)), weights=torch.as_tensor(area.values)
        )
        .sqrt()
        .cpu()
        .numpy()
        .item()
    )


def compute_pattern_biases(data: np.ndarray, target_index: int) -> List[np.ndarray]:
    biases = []
    for i in range(data.shape[0]):
        biases.append(data[i] - data[target_index])
    return biases


def compute_rmse(
    data: np.ndarray, area: xr.DataArray, target_index: int
) -> List[float]:
    biases = compute_pattern_biases(data, target_index)
    rmses = []
    for i in range(len(biases)):
        rmses.append(compute_pattern_rmse(biases[i], area))
    return rmses


def plot_rmse(config: Config, area: xr.DataArray, data, varname, ax: Axes):
    rmse = compute_rmse(data, area, config.target_index)
    rmse_std_10_years = config.rmse_std_ds.sel(window_size=10)[varname].values
    rmse_mean_10_years = config.rmse_mean_ds.sel(window_size=10)[varname].values
    ax.errorbar(config.weights, rmse, yerr=rmse_std_10_years * 2 / (50**0.5))
    ax.axhline(
        y=rmse_mean_10_years * 2**0.5 / (50**0.5),
        color="black",
        linestyle="--",
        label="noise floor",
    )
    ax.legend(loc="best")
    ax.set_title(varname)
    ax.set_xlabel("interpolant")
    ax.set_ylabel("pattern rmse")


def plot_rmse_over_interpolant(
    config: Config, time_mean: xr.Dataset, area: xr.DataArray
):
    fig, ax = plt.subplots(3, 3, figsize=(12, 8))
    for varname, axi in [
        ("surface_temperature", ax[0, 0]),
        ("air_temperature_7", ax[0, 1]),
        ("specific_total_water_7", ax[0, 2]),
        ("DLWRFsfc", ax[1, 0]),
        ("ULWRFtoa", ax[1, 1]),
        ("DSWRFsfc", ax[1, 2]),
        ("LHTFLsfc", ax[2, 0]),
        ("SHTFLsfc", ax[2, 1]),
        ("PRATEsfc", ax[2, 2]),
    ]:
        plot_rmse(config, area, time_mean[f"gen_map-{varname}"].values, varname, axi)
    plt.tight_layout()
    fig.savefig(os.path.join(config.output_dir, "rmse_over_interpolant.png"))


def plot_biases_over_interpolant(config: Config, time_mean: xr.Dataset):
    for var in config.plot_variables:
        plot_bias_over_interpolant(var, config, time_mean)


def plot_bias_over_interpolant(
    var: PlotVariable, config: Config, time_mean: xr.Dataset
):
    ncol = int(np.ceil(len(config.weights) / 4))
    varname = var.name
    fig, ax = plt.subplots(4, ncol, figsize=(4 * ncol, 13))
    fig.suptitle(
        "{} from interpolant {:.2f} to {:.2f}".format(
            varname, config.weights[0], config.weights[10]
        )
    )
    ax = ax.flatten()
    for i in range(len(config.weights)):
        (
            (
                time_mean[f"gen_map-{varname}"].isel(interp=i)
                - time_mean[f"gen_map-{varname}"].isel(interp=config.target_index)
            )
            * var.scale
        ).plot(
            ax=ax[i],
            cmap="RdBu",
            vmin=var.bias_vmin,
            vmax=var.bias_vmax,
        )
        ax[i].set_title(f"interp = {config.weights[i]}")
    plt.tight_layout()
    fig.savefig(
        os.path.join(config.output_dir, f"{var.name}_bias_over_interpolant.png")
    )


def regress(x, y):
    """
    Compute the slope of a linear regression, fitting both slope and intercept.
    """
    # Compute the means
    x_mean = np.mean(x)
    y_mean = np.mean(y, axis=0)

    # Compute numerator and denominator for slope
    numerator = np.sum((x - x_mean)[:, np.newaxis, np.newaxis] * (y - y_mean), axis=0)
    denominator = np.sum((x - x_mean) ** 2)

    # Compute slope
    slope = numerator / denominator
    return slope


def get_linear_variance(slope, map_):
    # Step 1: Flatten the arrays
    slope_flat = slope.flatten()
    map_flat = map_.flatten()

    # Step 2: Subtract the mean from slope and map_
    slope_mean = np.mean(slope_flat)
    map_mean = np.mean(map_flat)
    slope_flat_centered = slope_flat - slope_mean
    map_flat_centered = map_flat - map_mean

    # Step 3: Perform linear regression (slope prediction)
    # Find the linear relationship between slope and map (covariance-related)
    A = np.vstack(
        [slope_flat_centered, np.ones(len(slope_flat_centered))]
    ).T  # Add intercept term
    coefficients, _, _, _ = np.linalg.lstsq(A, map_flat_centered, rcond=None)
    linear_component_centered = A @ coefficients  # Predicted linear component

    # Step 4: Reshape the linear component to original shape
    linear_component_centered = linear_component_centered.reshape(slope.shape)

    # Step 5: Compute nonlinear component as the residual
    linear_component = linear_component_centered
    nonlinear_component = map_ - map_mean - linear_component

    # Step 6: Variance decomposition
    total_variance = np.var(map_flat_centered)
    linear_variance = np.var(linear_component)
    nonlinear_variance = np.var(nonlinear_component)
    return total_variance, linear_variance, nonlinear_variance


def get_variances(
    time_means: np.ndarray, noise_floor: float, slope: np.ndarray, target_index: int
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    var_linear = []
    var_total = []
    var_noise_floor = []
    var_nonlinear = []
    for i in range(time_means.shape[0]):
        total_variance, linear_variance, nonlinear_variance = get_linear_variance(
            slope, time_means[i] - time_means[target_index]
        )
        nonlinear_variance -= noise_floor**2
        var_noise_floor.append(noise_floor**2)
        var_linear.append(linear_variance)
        var_total.append(total_variance)
        var_nonlinear.append(nonlinear_variance)
    var_linear[target_index] = 0.0
    var_total[target_index] = 0.0
    var_noise_floor[target_index] = 0.0
    var_nonlinear[target_index] = 0.0
    return (
        np.asarray(var_linear),
        np.asarray(var_total),
        np.asarray(var_noise_floor),
        np.asarray(var_nonlinear),
    )


def linear_fit_analysis(config: Config, time_mean: xr.Dataset):
    for var in config.plot_variables:
        linear_fit_analysis_var(var, config, time_mean)


def linear_fit_analysis_var(
    var: PlotVariable, config: Config, time_mean_ds: xr.Dataset
):
    time_means = time_mean_ds[f"gen_map-{var.name}"].values * var.scale
    slope = regress(
        np.asarray(config.weights[config.fit_slice]),
        time_means[config.fit_slice] - time_means[config.target_index],
    )
    analysis_from_slope(slope, time_means, var, config, "regression")
    i_start = config.linear_fit.i_start
    if i_start is None:
        i_start = 0
    i_end = config.linear_fit.i_end
    if i_end is None:
        i_end = len(config.weights)
    slope = time_means[i_end - 1] - time_means[i_start]
    analysis_from_slope(slope, time_means, var, config, "window_difference")


def analysis_from_slope(
    slope: np.ndarray,
    time_means: np.ndarray,
    var: PlotVariable,
    config: Config,
    label: str,
):
    plt.figure(figsize=(6, 4))
    i_start = config.linear_fit.i_start
    if i_start is None:
        i_start = 0
    i_end = config.linear_fit.i_end
    if i_end is None:
        i_end = len(config.weights)
    plt.title(
        "Linear fit of {}\nfrom interpolant {:.2f} to {:.2f}".format(
            var.name, config.weights[i_start], config.weights[i_end - 1]
        )
    )
    vmin = min(slope.min(), -slope.max())
    vmax = -vmin
    im = plt.pcolormesh(slope, cmap="RdBu", vmin=vmin, vmax=vmax)
    plt.colorbar(im)
    plt.xlabel("lon")
    plt.ylabel("lat")
    plt.tight_layout()
    plt.savefig(os.path.join(config.output_dir, f"{var.name}_{label}_linear_fit.png"))
    noise_floor = (
        config.rmse_mean_ds.sel(window_size=10)[var.name].values
        * 2**0.5
        / ((config.inference_years / 10.0) ** 0.5)
        * var.scale
    )
    print("Noise floor for {} is {}".format(var.name, noise_floor))
    var_linear, var_total, var_noise_floor, var_nonlinear = get_variances(
        time_means,
        noise_floor,
        slope,
        config.target_index,
    )

    # Number of bars
    n_bars = len(var_linear)
    x = np.arange(n_bars)
    x_labels = np.asarray(config.weights)

    fig, ax = plt.subplots(figsize=(6, 4))

    # Plot var_noise_floor at the bottom
    ax.bar(x, var_noise_floor, label="Noise Floor", color="skyblue")

    linear_bottom = var_noise_floor.copy()
    linear_bottom[var_linear < 0] = 0.0
    # Plot var_linear on top of var_noise_floor
    ax.bar(x, var_linear, bottom=linear_bottom, label="Linear Variance", color="salmon")

    # Calculate cumulative bottom for var_nonlinear
    nonlinear_bottom = var_noise_floor.copy()
    nonlinear_bottom[var_linear > 0] += var_linear[var_linear > 0]
    nonlinear_bottom[var_nonlinear < 0] = 0.0

    # # Plot var_nonlinear on top
    ax.bar(
        x,
        var_nonlinear,
        bottom=nonlinear_bottom,
        label="Nonlinear Variance",
        color="lightgreen",
    )

    # Set labels and title
    ax.set_xlabel("interpolant")
    ax.set_ylabel("Variance")
    ax.set_title(f"Variance Components of {var.name}")

    # Set x-axis ticks (optional)
    ax.set_xticks(x)
    ax.set_xticklabels(["{:.2f}".format(i) for i in x_labels])
    ax.plot(x, var_total, linestyle="", marker=".", label="Total variance")

    # Add legend
    ax.legend()

    plt.tight_layout()
    fig.savefig(
        os.path.join(config.output_dir, f"{var.name}_{label}_variance_components.png")
    )


def main(config: Config):
    if not os.path.exists(DATASETS_DIR):
        os.makedirs(DATASETS_DIR)

    for name in config.beaker_datasets:
        if not os.path.exists(os.path.join(DATASETS_DIR, name)):
            print("Downloading dataset", name)
            download_beaker_time_mean_diagnostics(name)

    time_mean_datasets = []
    for name in config.beaker_datasets:
        time_mean_datasets.append(
            xr.open_dataset(f"interp-datasets/{name}/time_mean_diagnostics.nc")
        )
    time_mean: xr.Dataset = xr.concat(time_mean_datasets, dim="interp")
    time_mean = time_mean.assign_coords({"interp": config.weights})
    lat = time_mean["lat"]
    area = xr.DataArray(
        metrics.spherical_area_weights(lat.values, 90),
        dims=["lat", "lon"],
    )
    os.makedirs(config.output_dir, exist_ok=True)
    linear_fit_analysis(config, time_mean)
    plot_rmse_over_interpolant(config, time_mean, area)
    plot_biases_over_interpolant(config, time_mean)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("config", type=str)
    args = parser.parse_args()
    with open(args.config, "r") as f:
        data = yaml.safe_load(f)
    config = dacite.from_dict(
        data_class=Config, data=data, config=dacite.Config(strict=True)
    )
    main(config)
